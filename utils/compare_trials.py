import matplotlib.pyplot as plt
import numpy as np
import matplotlib.colors as mcolors
# Extract test accuracies from each experiment and round to 3 significant figures
experiments_data = [
    # Experiment 1
    [0.006, 0.0358, 0.0575, 0.0748, 0.084, 0.0887, 0.102, 0.106, 0.111, 0.1176, 0.1246, 0.1298, 0.135, 0.1411, 0.1489, 0.1527, 0.1582, 0.1628, 0.1694, 0.1767, 0.1809, 0.1856, 0.1849, 0.1851, 0.1863, 0.1874, 0.1858, 0.184, 0.1864, 0.1894, 0.1887, 0.1867, 0.19, 0.1879, 0.1905, 0.1896, 0.1891, 0.1896, 0.1883, 0.1889],
    
    # Experiment 2
    [0.006, 0.1336, 0.2, 0.2384, 0.2661, 0.2941, 0.3157, 0.3335, 0.3507, 0.3626, 0.3722, 0.3869, 0.3944, 0.4132, 0.4172, 0.4234, 0.439, 0.4449, 0.4557, 0.453, 0.4648, 0.4715, 0.4744, 0.4735, 0.4763, 0.478, 0.4804, 0.482, 0.4835, 0.4886, 0.4913, 0.4896, 0.4881, 0.4881, 0.4902, 0.489, 0.4909, 0.4898, 0.49, 0.4897],
    
    # Experiment 3
    [0.006, 0.0477, 0.0811, 0.098, 0.1089, 0.1188, 0.1277, 0.1372, 0.1469, 0.1566, 0.1692, 0.177, 0.1841, 0.1918, 0.1993, 0.2055, 0.2131, 0.2172, 0.2239, 0.2303, 0.2385, 0.2404, 0.2426, 0.2422, 0.2434, 0.2449, 0.2433, 0.246, 0.2461, 0.2454, 0.247, 0.2478, 0.2479, 0.2458, 0.2473, 0.2481, 0.2443, 0.2478, 0.2469, 0.2478, 0.2474],
    
    # Experiment 4
    [0.006, 0.1275, 0.2116, 0.2854, 0.3457, 0.3907, 0.4359, 0.4671, 0.489, 0.5143, 0.5356, 0.5534, 0.5735, 0.5779, 0.5963, 0.5967, 0.6124, 0.6188, 0.6175, 0.6331, 0.6311, 0.6574, 0.6642, 0.6643, 0.6641, 0.6669, 0.6687, 0.6677, 0.67, 0.67, 0.6704, 0.6684, 0.67, 0.6708, 0.6701, 0.6734, 0.6704, 0.6705, 0.6714, 0.6712, 0.6722],
    
    # Experiment 5
    [0.006, 0.2961, 0.4705, 0.4869, 0.568, 0.5879, 0.6246, 0.6046, 0.631, 0.6538, 0.6671, 0.6733, 0.6714, 0.6643, 0.679, 0.6879, 0.6882, 0.6855, 0.6821, 0.695, 0.6883, 0.727, 0.7316, 0.7344, 0.7316, 0.7328, 0.7335, 0.7341, 0.7323, 0.7321, 0.7355, 0.7364, 0.736, 0.7363, 0.7374, 0.7373, 0.7367, 0.7357, 0.7355, 0.7357, 0.7378],
    
    # Experiment 6
    [0.006, 0.2129, 0.3669, 0.4223, 0.4867, 0.5294, 0.5535, 0.576, 0.5946, 0.5957, 0.6241, 0.6243, 0.6378, 0.6442, 0.6331, 0.6547, 0.6548, 0.6583, 0.6555, 0.6644, 0.6616, 0.6864, 0.6899, 0.6891, 0.6919, 0.6914, 0.688, 0.6878, 0.6867, 0.6877, 0.6883, 0.6927, 0.6923, 0.6917, 0.6884, 0.6914, 0.6926, 0.6915, 0.6897, 0.6904, 0.691],
    
    # Experiment 7
    [0.006, 0.0483, 0.075, 0.0845, 0.0935, 0.104, 0.1066, 0.1136, 0.1241, 0.1302, 0.1349, 0.14, 0.1423, 0.1506, 0.1547, 0.1576, 0.1665, 0.1706, 0.1715, 0.178, 0.1799, 0.1818, 0.1822, 0.1836, 0.188, 0.1856, 0.1869, 0.1865, 0.1856, 0.1879, 0.1859, 0.1879, 0.1871, 0.1874, 0.1881, 0.1854, 0.1871, 0.1873, 0.1888, 0.1857, 0.1897],
    
    # Experiment 8
    [0.006, 0.0781, 0.1089, 0.1373, 0.1515, 0.1734, 0.1928, 0.2111, 0.2231, 0.237, 0.2531, 0.268, 0.2843, 0.2994, 0.3078, 0.32, 0.3319, 0.3433, 0.3556, 0.3644, 0.3761, 0.3812, 0.3831, 0.3829, 0.3864, 0.3886, 0.389, 0.3894, 0.3926, 0.3924, 0.3944, 0.3931, 0.3949, 0.3942, 0.3951, 0.3944, 0.3937, 0.3929, 0.3936, 0.3947, 0.3941],
    
    # Experiment 9
    [0.006, 0.094, 0.1305, 0.1498, 0.1744, 0.2031, 0.2209, 0.2345, 0.2522, 0.2686, 0.2766, 0.2938, 0.3042, 0.3135, 0.3241, 0.3312, 0.3429, 0.3503, 0.358, 0.3668, 0.3775, 0.3833, 0.3878, 0.387, 0.3871, 0.3871, 0.3925, 0.3933, 0.3939, 0.3965, 0.3947, 0.3952, 0.3952, 0.3988, 0.4, 0.4011, 0.4021, 0.4008, 0.4022, 0.4005, 0.4011],
    
    # Experiment 10
    [0.006, 0.148, 0.2099, 0.2472, 0.2751, 0.3027, 0.3252, 0.3403, 0.3586, 0.3685, 0.3885, 0.4032, 0.4104, 0.4199, 0.4257, 0.4387, 0.4539, 0.4618, 0.466, 0.4733, 0.4831, 0.4911, 0.4945, 0.4966, 0.4963, 0.4999, 0.5, 0.5015, 0.5038, 0.506, 0.5055, 0.5099, 0.5091, 0.5096, 0.5095, 0.51, 0.5098, 0.5106, 0.5096, 0.5124, 0.5117],
    
    # Experiment 11
    [0.006, 0.1459, 0.2648, 0.3611, 0.4181, 0.4711, 0.5058, 0.5297, 0.553, 0.5751, 0.5876, 0.5995, 0.6122, 0.6153, 0.6219, 0.6282, 0.6398, 0.6449, 0.6477, 0.6507, 0.6501, 0.6702, 0.6735, 0.6759, 0.6759, 0.6766, 0.677, 0.6774, 0.6782, 0.6812, 0.6772, 0.6807, 0.6796, 0.6778, 0.6781, 0.6792, 0.678, 0.6804, 0.6796, 0.6794, 0.677],
    
    # Experiment 12
    [0.006, 0.2305, 0.387, 0.4716, 0.5305, 0.567, 0.5916, 0.6176, 0.6299, 0.6447, 0.6619, 0.6635, 0.6672, 0.6803, 0.671, 0.6771, 0.6761, 0.6841, 0.6855, 0.6915, 0.6907, 0.709, 0.7137, 0.7121, 0.7157, 0.7148, 0.7174, 0.7135, 0.7154, 0.7171, 0.7147, 0.7157, 0.716, 0.717, 0.717, 0.716, 0.7166, 0.7173, 0.7168, 0.7164, 0.7165],
    
    # Experiment 13
    [0.006, 0.0656, 0.0938, 0.1096, 0.1251, 0.1382, 0.1492, 0.1648, 0.1764, 0.185, 0.1994, 0.2062, 0.2179, 0.2321, 0.2391, 0.2484, 0.2568, 0.2683, 0.2784, 0.285, 0.2923, 0.2979, 0.2982, 0.2994, 0.3, 0.3041, 0.3042, 0.3062, 0.3035, 0.3057, 0.3088, 0.3075, 0.3082, 0.3085, 0.3081, 0.3089, 0.3077, 0.309, 0.3081, 0.3085, 0.3081],
    
    # Experiment 14
    [0.006, 0.1651, 0.2596, 0.3359, 0.4103, 0.464, 0.5063, 0.5506, 0.5677, 0.5867, 0.5985, 0.6066, 0.616, 0.6213, 0.6258, 0.6327, 0.6282, 0.6379, 0.6372, 0.641, 0.6419, 0.6603, 0.6694, 0.6743, 0.6771, 0.6749, 0.6759, 0.6756, 0.6742, 0.6763, 0.6759, 0.6756, 0.6778, 0.678, 0.6792, 0.6804, 0.6795, 0.682, 0.678, 0.681, 0.6795],
    
    # Experiment 15
    [0.006, 0.1747, 0.297, 0.3953, 0.4602, 0.5165, 0.538, 0.5677, 0.5825, 0.5968, 0.6227, 0.6265, 0.6352, 0.6424, 0.6499, 0.6551, 0.6659, 0.663, 0.6611, 0.6661, 0.6702, 0.6888, 0.6967, 0.6977, 0.6986, 0.697, 0.6992, 0.7015, 0.7004, 0.7016, 0.6999, 0.6997, 0.7013, 0.7009, 0.7013, 0.7014, 0.7003, 0.7028, 0.7015, 0.7017, 0.7011],
    
    # Experiment 16
    [0.006, 0.1541, 0.2334, 0.2895, 0.3405, 0.4011, 0.4384, 0.4882, 0.5243, 0.5479, 0.5738, 0.6006, 0.6034, 0.6131, 0.6218, 0.6286, 0.6331, 0.637, 0.6318, 0.6384, 0.6486, 0.6768, 0.6911, 0.6926, 0.6897, 0.6925, 0.6956, 0.6939, 0.6955, 0.6967, 0.693, 0.6952, 0.6977, 0.6988, 0.6979, 0.6983, 0.6973, 0.6997, 0.6983, 0.6973, 0.6972],
    
    # Experiment 17
    [0.006, 0.1507, 0.2647, 0.3406, 0.4061, 0.4445, 0.4812, 0.5115, 0.5291, 0.5417, 0.5599, 0.5863, 0.5925, 0.6001, 0.6173, 0.6285, 0.6217, 0.635, 0.634, 0.6397, 0.6422, 0.6684, 0.6686, 0.6685, 0.6728, 0.6699, 0.67, 0.6712, 0.6686, 0.6745, 0.6729, 0.6716, 0.6731, 0.6736, 0.6743, 0.6758, 0.6729, 0.6763, 0.6751, 0.6749, 0.6751],

    # Experiment 18
    [0.006, 0.1046, 0.1558, 0.2098, 0.2413, 0.2812, 0.31, 0.3442, 0.373, 0.4027, 0.4282, 0.4512, 0.4754, 0.4904, 0.5115, 0.528, 0.5453, 0.559, 0.569, 0.5756, 0.594, 0.6029, 0.6134, 0.6131, 0.6198, 0.6177, 0.6243, 0.6267, 0.6273, 0.628, 0.6297, 0.6301, 0.6307, 0.6313, 0.6323, 0.6333, 0.6315, 0.6326, 0.6309, 0.6318, 0.6341],

    # Experiment 19
    [0.006, 0.1626, 0.2119, 0.2553, 0.2798, 0.3108, 0.3332, 0.3627, 0.3881, 0.4094, 0.4284, 0.449, 0.4667, 0.4865, 0.506, 0.5205, 0.5399, 0.551, 0.5606, 0.5776, 0.5887, 0.6007, 0.6018, 0.6079, 0.6094, 0.6155, 0.6219, 0.619, 0.6245, 0.6259, 0.6257, 0.6301, 0.6319, 0.6316, 0.6314, 0.633, 0.6318, 0.6318, 0.6307, 0.6328, 0.6344],

    # Experiment 20
    [0.006, 0.2361, 0.3941, 0.494, 0.5352, 0.5855, 0.6134, 0.6353, 0.6439, 0.6409, 0.6491, 0.6604, 0.6503, 0.6581, 0.6564, 0.6619, 0.6585, 0.6609, 0.6629, 0.6577, 0.6666, 0.6958, 0.7027, 0.7019, 0.7027, 0.7044, 0.7042, 0.7061, 0.7058, 0.7048, 0.7064, 0.7074, 0.7073, 0.708, 0.7082, 0.7065, 0.7073, 0.7074, 0.7067, 0.709, 0.7088]]


# Extract test accuracies and learning rates from each experiment
experiments_data_dict = {
    'Exp_1': {
        'learning_rate': 1.57e-05,
        'test_accuracies': [0.006, 0.0358, 0.0575, 0.0748, 0.084, 0.0887, 0.102, 0.106, 0.111, 0.1176, 0.1246, 0.1298, 0.135, 0.1411, 0.1489, 0.1527, 0.1582, 0.1628, 0.1694, 0.1767, 0.1809, 0.1856, 0.1849, 0.1851, 0.1863, 0.1874, 0.1858, 0.184, 0.1864, 0.1894, 0.1887, 0.1867, 0.19, 0.1879, 0.1905, 0.1896, 0.1891, 0.1896, 0.1883, 0.1889]
    },
    'Exp_2': {
        'learning_rate': 0.00336,
        'test_accuracies': [0.006, 0.1336, 0.2, 0.2384, 0.2661, 0.2941, 0.3157, 0.3335, 0.3507, 0.3626, 0.3722, 0.3869, 0.3944, 0.4132, 0.4172, 0.4234, 0.439, 0.4449, 0.4557, 0.453, 0.4648, 0.4715, 0.4744, 0.4735, 0.4763, 0.478, 0.4804, 0.482, 0.4835, 0.4886, 0.4913, 0.4896, 0.4881, 0.4881, 0.4902, 0.489, 0.4909, 0.4898, 0.49, 0.4897]
    },
    'Exp_3': {
        'learning_rate': 2.14e-05,
        'test_accuracies': [0.006, 0.0477, 0.0811, 0.098, 0.1089, 0.1188, 0.1277, 0.1372, 0.1469, 0.1566, 0.1692, 0.177, 0.1841, 0.1918, 0.1993, 0.2055, 0.2131, 0.2172, 0.2239, 0.2303, 0.2385, 0.2404, 0.2426, 0.2422, 0.2434, 0.2449, 0.2433, 0.246, 0.2461, 0.2454, 0.247, 0.2478, 0.2479, 0.2458, 0.2473, 0.2481, 0.2443, 0.2478, 0.2469, 0.2478, 0.2474]
    },
    'Exp_4': {
        'learning_rate': 0.000463,
        'test_accuracies': [0.006, 0.1275, 0.2116, 0.2854, 0.3457, 0.3907, 0.4359, 0.4671, 0.489, 0.5143, 0.5356, 0.5534, 0.5735, 0.5779, 0.5963, 0.5967, 0.6124, 0.6188, 0.6175, 0.6331, 0.6311, 0.6574, 0.6642, 0.6643, 0.6641, 0.6669, 0.6687, 0.6677, 0.67, 0.67, 0.6704, 0.6684, 0.67, 0.6708, 0.6701, 0.6734, 0.6704, 0.6705, 0.6714, 0.6712, 0.6722]
    },
    'Exp_5': {
        'learning_rate': 0.00783,
        'test_accuracies': [0.006, 0.2961, 0.4705, 0.4869, 0.568, 0.5879, 0.6246, 0.6046, 0.631, 0.6538, 0.6671, 0.6733, 0.6714, 0.6643, 0.679, 0.6879, 0.6882, 0.6855, 0.6821, 0.695, 0.6883, 0.727, 0.7316, 0.7344, 0.7316, 0.7328, 0.7335, 0.7341, 0.7323, 0.7321, 0.7355, 0.7364, 0.736, 0.7363, 0.7374, 0.7373, 0.7367, 0.7357, 0.7355, 0.7357, 0.7378]
    },
    'Exp_6': {
        'learning_rate': 0.000417,
        'test_accuracies': [0.006, 0.2129, 0.3669, 0.4223, 0.4867, 0.5294, 0.5535, 0.576, 0.5946, 0.5957, 0.6241, 0.6243, 0.6378, 0.6442, 0.6331, 0.6547, 0.6548, 0.6583, 0.6555, 0.6644, 0.6616, 0.6864, 0.6899, 0.6891, 0.6919, 0.6914, 0.688, 0.6878, 0.6867, 0.6877, 0.6883, 0.6927, 0.6923, 0.6917, 0.6884, 0.6914, 0.6926, 0.6915, 0.6897, 0.6904, 0.691]
    },
    'Exp_7': {
        'learning_rate': 1.99e-05,
        'test_accuracies': [0.006, 0.0483, 0.075, 0.0845, 0.0935, 0.104, 0.1066, 0.1136, 0.1241, 0.1302, 0.1349, 0.14, 0.1423, 0.1506, 0.1547, 0.1576, 0.1665, 0.1706, 0.1715, 0.178, 0.1799, 0.1818, 0.1822, 0.1836, 0.188, 0.1856, 0.1869, 0.1865, 0.1856, 0.1879, 0.1859, 0.1879, 0.1871, 0.1874, 0.1881, 0.1854, 0.1871, 0.1873, 0.1888, 0.1857, 0.1897]
    },
    'Exp_8': {
        'learning_rate': 6.26e-05,
        'test_accuracies': [0.006, 0.0781, 0.1089, 0.1373, 0.1515, 0.1734, 0.1928, 0.2111, 0.2231, 0.237, 0.2531, 0.268, 0.2843, 0.2994, 0.3078, 0.32, 0.3319, 0.3433, 0.3556, 0.3644, 0.3761, 0.3812, 0.3831, 0.3829, 0.3864, 0.3886, 0.389, 0.3894, 0.3926, 0.3924, 0.3944, 0.3931, 0.3949, 0.3942, 0.3951, 0.3944, 0.3937, 0.3929, 0.3936, 0.3947, 0.3941]
    },
    'Exp_9': {
        'learning_rate': 0.000329,
        'test_accuracies': [0.006, 0.094, 0.1305, 0.1498, 0.1744, 0.2031, 0.2209, 0.2345, 0.2522, 0.2686, 0.2766, 0.2938, 0.3042, 0.3135, 0.3241, 0.3312, 0.3429, 0.3503, 0.358, 0.3668, 0.3775, 0.3833, 0.3878, 0.387, 0.3871, 0.3871, 0.3925, 0.3933, 0.3939, 0.3965, 0.3947, 0.3952, 0.3952, 0.3988, 0.4, 0.4011, 0.4021, 0.4008, 0.4022, 0.4005, 0.4011]
    },
    'Exp_10': {
        'learning_rate': 0.00192,
        'test_accuracies': [0.006, 0.148, 0.2099, 0.2472, 0.2751, 0.3027, 0.3252, 0.3403, 0.3586, 0.3685, 0.3885, 0.4032, 0.4104, 0.4199, 0.4257, 0.4387, 0.4539, 0.4618, 0.466, 0.4733, 0.4831, 0.4911, 0.4945, 0.4966, 0.4963, 0.4999, 0.5, 0.5015, 0.5038, 0.506, 0.5055, 0.5099, 0.5091, 0.5096, 0.5095, 0.51, 0.5098, 0.5106, 0.5096, 0.5124, 0.5117]
    },
    'Exp_11': {
        'learning_rate': 0.000373,
        'test_accuracies': [0.006, 0.1459, 0.2648, 0.3611, 0.4181, 0.4711, 0.5058, 0.5297, 0.553, 0.5751, 0.5876, 0.5995, 0.6122, 0.6153, 0.6219, 0.6282, 0.6398, 0.6449, 0.6477, 0.6507, 0.6501, 0.6702, 0.6735, 0.6759, 0.6759, 0.6766, 0.677, 0.6774, 0.6782, 0.6812, 0.6772, 0.6807, 0.6796, 0.6778, 0.6781, 0.6792, 0.678, 0.6804, 0.6796, 0.6794, 0.677]
    },
    'Exp_12': {
        'learning_rate': 0.00147,
        'test_accuracies': [0.006, 0.2305, 0.387, 0.4716, 0.5305, 0.567, 0.5916, 0.6176, 0.6299, 0.6447, 0.6619, 0.6635, 0.6672, 0.6803, 0.671, 0.6771, 0.6761, 0.6841, 0.6855, 0.6915, 0.6907, 0.709, 0.7137, 0.7121, 0.7157, 0.7148, 0.7174, 0.7135, 0.7154, 0.7171, 0.7147, 0.7157, 0.716, 0.717, 0.717, 0.716, 0.7166, 0.7173, 0.7168, 0.7164, 0.7165]
    },
    'Exp_13': {
        'learning_rate': 3.10e-05,
        'test_accuracies': [0.006, 0.0656, 0.0938, 0.1096, 0.1251, 0.1382, 0.1492, 0.1648, 0.1764, 0.185, 0.1994, 0.2062, 0.2179, 0.2321, 0.2391, 0.2484, 0.2568, 0.2683, 0.2784, 0.285, 0.2923, 0.2979, 0.2982, 0.2994, 0.3, 0.3041, 0.3042, 0.3062, 0.3035, 0.3057, 0.3088, 0.3075, 0.3082, 0.3085, 0.3081, 0.3089, 0.3077, 0.309, 0.3081, 0.3085, 0.3081]
    },
    'Exp_14': {
        'learning_rate': 0.0966,
        'test_accuracies': [0.006, 0.1651, 0.2596, 0.3359, 0.4103, 0.464, 0.5063, 0.5506, 0.5677, 0.5867, 0.5985, 0.6066, 0.616, 0.6213, 0.6258, 0.6327, 0.6282, 0.6379, 0.6372, 0.641, 0.6419, 0.6603, 0.6694, 0.6743, 0.6771, 0.6749, 0.6759, 0.6756, 0.6742, 0.6763, 0.6759, 0.6756, 0.6778, 0.678, 0.6792, 0.6804, 0.6795, 0.682, 0.678, 0.681, 0.6795]
    },
    'Exp_15': {
        'learning_rate': 0.00105,
        'test_accuracies': [0.006, 0.1747, 0.297, 0.3953, 0.4602, 0.5165, 0.538, 0.5677, 0.5825, 0.5968, 0.6227, 0.6265, 0.6352, 0.6424, 0.6499, 0.6551, 0.6659, 0.663, 0.6611, 0.6661, 0.6702, 0.6888, 0.6967, 0.6977, 0.6986, 0.697, 0.6992, 0.7015, 0.7004, 0.7016, 0.6999, 0.6997, 0.7013, 0.7009, 0.7013, 0.7014, 0.7003, 0.7028, 0.7015, 0.7017, 0.7011],
    
    },
    'Exp_16': {
        'learning_rate': 0.031256,
        'test_accuracies': [0.006, 0.1541, 0.2334, 0.2895, 0.3405, 0.4011, 0.4384, 0.4882, 0.5243, 0.5479, 0.5738, 0.6006, 0.6034, 0.6131, 0.6218, 0.6286, 0.6331, 0.637, 0.6318, 0.6384, 0.6486, 0.6768, 0.6911, 0.6926, 0.6897, 0.6925, 0.6956, 0.6939, 0.6955, 0.6967, 0.693, 0.6952, 0.6977, 0.6988, 0.6979, 0.6983, 0.6973, 0.6997, 0.6983, 0.6973, 0.6972],
    
    },
    'Exp_17': {
        'learning_rate': 0.000514289276,
        'test_accuracies': [0.006, 0.1507, 0.2647, 0.3406, 0.4061, 0.4445, 0.4812, 0.5115, 0.5291, 0.5417, 0.5599, 0.5863, 0.5925, 0.6001, 0.6173, 0.6285, 0.6217, 0.635, 0.634, 0.6397, 0.6422, 0.6684, 0.6686, 0.6685, 0.6728, 0.6699, 0.67, 0.6712, 0.6686, 0.6745, 0.6729, 0.6716, 0.6731, 0.6736, 0.6743, 0.6758, 0.6729, 0.6763, 0.6751, 0.6749, 0.6751],

    },
    'Exp_18': {
        'learning_rate': 0.0007100178,
        'test_accuracies': [0.006, 0.1046, 0.1558, 0.2098, 0.2413, 0.2812, 0.31, 0.3442, 0.373, 0.4027, 0.4282, 0.4512, 0.4754, 0.4904, 0.5115, 0.528, 0.5453, 0.559, 0.569, 0.5756, 0.594, 0.6029, 0.6134, 0.6131, 0.6198, 0.6177, 0.6243, 0.6267, 0.6273, 0.628, 0.6297, 0.6301, 0.6307, 0.6313, 0.6323, 0.6333, 0.6315, 0.6326, 0.6309, 0.6318, 0.6341],

    },
    'Exp_19': {
        'learning_rate': 0.024194,
        'test_accuracies': [0.006, 0.1626, 0.2119, 0.2553, 0.2798, 0.3108, 0.3332, 0.3627, 0.3881, 0.4094, 0.4284, 0.449, 0.4667, 0.4865, 0.506, 0.5205, 0.5399, 0.551, 0.5606, 0.5776, 0.5887, 0.6007, 0.6018, 0.6079, 0.6094, 0.6155, 0.6219, 0.619, 0.6245, 0.6259, 0.6257, 0.6301, 0.6319, 0.6316, 0.6314, 0.633, 0.6318, 0.6318, 0.6307, 0.6328, 0.6344],

    },
    'Exp_20': {
        'learning_rate': 0.054699,
        'test_accuracies': [0.006, 0.2361, 0.3941, 0.494, 0.5352, 0.5855, 0.6134, 0.6353, 0.6439, 0.6409, 0.6491, 0.6604, 0.6503, 0.6581, 0.6564, 0.6619, 0.6585, 0.6609, 0.6629, 0.6577, 0.6666, 0.6958, 0.7027, 0.7019, 0.7027, 0.7044, 0.7042, 0.7061, 0.7058, 0.7048, 0.7064, 0.7074, 0.7073, 0.708, 0.7082, 0.7065, 0.7073, 0.7074, 0.7067, 0.709, 0.7088]

    }}
ig, ax = plt.subplots()

# Create a colormap and normalize trial numbers
n_trials = len(experiments_data_dict.values())

cmap = plt.cm.plasma  # You can change this to other colormaps like 'plasma', 'coolwarm', etc.

#colors = cmap(np.linspace(0, 1, n_trials))
learning_rates = []
for i in range(len(experiments_data_dict)):
    trial = experiments_data_dict[f"Exp_{i+1}"]
    learning_rates.append(trial['learning_rate'])
# Create log normalization
norm = mcolors.LogNorm(vmin=min(learning_rates), vmax=max(learning_rates))

# Map learning rates to colors using log norm
colors = cmap(norm(learning_rates))
# Plot each trial with its corresponding color
for i in range(n_trials):
    lr = experiments_data_dict[f"Exp_{i+1}"]['learning_rate']
    ax.plot(range(len(experiments_data[i])), experiments_data[i], 
            color=colors[i], label=f"Trial {i}")

ax.set_ylim([0.0, 0.75])
ax.set_xlabel('Steps (x1000)')
ax.set_ylabel('Test Accuracy')


sm = plt.cm.ScalarMappable(
    cmap=cmap,
    norm=mcolors.LogNorm(vmin=min(learning_rates), vmax=max(learning_rates))
)
sm.set_array([])

cbar = plt.colorbar(sm, ax=ax)
cbar.set_label('Learning Rate')

plt.savefig('Compare_target_trials_CIFAR100', dpi=500)
plt.show()